{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d545b1f-0b22-4588-9a43-819b3d955ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Importing standard Qiskit libraries\n",
    "from qiskit import QuantumCircuit, transpile, Aer, IBMQ\n",
    "from qiskit.tools.jupyter import *\n",
    "from qiskit.visualization import *\n",
    "from ibm_quantum_widgets import *\n",
    "from qiskit.providers.aer import QasmSimulator\n",
    "\n",
    "# Loading your IBM Quantum account(s)\n",
    "provider = IBMQ.load_account()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bbd917a-87ff-4908-8689-b2925837645a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (2022.10.31)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.8/site-packages (from transformers) (1.22.4)\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\n",
      "  Using cached huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.8/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.8/site-packages (from transformers) (4.64.1)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.9.0-py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->transformers) (2.0.12)\n",
      "Installing collected packages: filelock, tokenizers, huggingface-hub, transformers\n",
      "Successfully installed filelock-3.9.0 huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n",
      "Collecting polars\n",
      "  Downloading polars-0.15.9-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 14.6 MB 108 kB/s  eta 0:00:01s eta 0:00:01��██▌           | 9.4 MB 24.6 MB/s eta 0:00:01 24.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing_extensions>=4.0.0 in /opt/conda/lib/python3.8/site-packages (from polars) (4.4.0)\n",
      "Installing collected packages: polars\n",
      "Successfully installed polars-0.15.9\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install polars\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import matplotlib.pyplot as plt\n",
    "import polars as pl\n",
    "import logging\n",
    "import timeit\n",
    "import string\n",
    "import re\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a3fe2e5-badd-4a6c-9df1-8254ec04509a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (10, 1)\n",
      "┌─────────────────────────────────────┐\n",
      "│ tokens                              │\n",
      "│ ---                                 │\n",
      "│ list[str]                           │\n",
      "╞═════════════════════════════════════╡\n",
      "│ []                                  │\n",
      "├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
      "│ []                                  │\n",
      "├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
      "│ [\"most\", \"people\", ... \"facility... │\n",
      "├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
      "│ []                                  │\n",
      "├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
      "│ ...                                 │\n",
      "├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
      "│ [\"this\", \"web\", ... \"tm\"]           │\n",
      "├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
      "│ [\"including\", \"how\", ... \"litera... │\n",
      "├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
      "│ [\"archive\", \"foundation\", ... \"t... │\n",
      "├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
      "│ [\"sub\", \"scribe\", ... \"s\"]          │\n",
      "└─────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Define a list of stop words\n",
    "stop_words = ['a', 'an', 'the', 'in', 'of', 'on']\n",
    "\n",
    "# Use a generator function to yield the data one batch at a time\n",
    "def process_text(file):\n",
    "    with open(file, encoding='utf-8-sig') as f:\n",
    "        # Process each line of the file\n",
    "        for line in f:\n",
    "            # Use a regular expression to remove punctuation and lowercase the text\n",
    "            line = re.sub(f'[{string.punctuation}]', '', line)\n",
    "            line = line.lower()\n",
    "            # Tokenize the text\n",
    "            tokens = tokenizer.tokenize(line)\n",
    "            # Use a regular expression to match numbers and special characters\n",
    "            pattern = r'[\\d@!#]+'\n",
    "            # Use the sub() function to remove the matched characters\n",
    "            tokens = [re.sub(pattern, '', token) for token in tokens]\n",
    "            # Use a list comprehension to filter the tokens\n",
    "            filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "            # Yield the modified tokens\n",
    "            yield filtered_tokens\n",
    "\n",
    "# Use the map function to apply the process_text function to each line of the file\n",
    "text_tokens = map(process_text, ['bible.txt'])\n",
    "\n",
    "# Use the itertools module to flatten the list of lists\n",
    "text_tokens = list(itertools.chain.from_iterable(text_tokens))\n",
    "\n",
    "# Create a Polars DataFrame from the list of tokens\n",
    "df = pl.DataFrame({'tokens': text_tokens})\n",
    "\n",
    "# Use the head() method to print the first 10 rows of the DataFrame\n",
    "print(df.tail(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f4578292-6239-440c-99de-bff3fa91fbdb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              tokens\n",
      "0  [11, in, the, beginning, god, created, the, he...\n",
      "1                                                 []\n",
      "2  [12, and, the, earth, was, without, form, and,...\n",
      "3  [the, face, of, the, deep, and, the, spirit, o...\n",
      "4                                           [waters]\n",
      "5                                                 []\n",
      "6  [13, and, god, said, let, there, be, light, an...\n",
      "7                                                 []\n",
      "8  [14, and, god, saw, the, light, that, it, was,...\n",
      "9                              [from, the, darkness]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "from itertools import chain\n",
    "import pprint\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Define a list of stop words\n",
    "stop_words = ['a', 'an', 'the', 'in', 'of', 'on']\n",
    "\n",
    "# Use a generator function to yield the data one batch at a time\n",
    "def process_text(file):\n",
    "    with open(file, encoding='utf-8-sig') as f:\n",
    "        # Use a regular expression to remove punctuation and lowercase the text\n",
    "        # Tokenize the text\n",
    "        # Use a regular expression to match numbers and special characters\n",
    "        # Use the sub() function to remove the matched characters\n",
    "        # Use a list comprehension to filter the tokens\n",
    "        # Yield the modified tokens\n",
    "        yield from (\n",
    "            tokenizer.tokenize(re.sub(f'[{string.punctuation}]', '', line.lower()))\n",
    "            for line in f\n",
    "        )\n",
    "\n",
    "# Use the map function to apply the process_text function to each line of the file\n",
    "# Use the itertools module to flatten the list of lists\n",
    "# Create a Pandas DataFrame from the list of tokens\n",
    "# Use the head() method to print the first 10 rows of the DataFrame\n",
    "df = pd.DataFrame({'tokens': chain.from_iterable(map(process_text, ['bible.txt']))})\n",
    "pprint.pprint(df.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d77cdfa7-5f23-4e55-8329-35484bbb27ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [11, in, the, beginning, god, created, the, he...\n",
      "1                                                   []\n",
      "2    [12, and, the, earth, wa, without, form, and, ...\n",
      "3    [the, face, of, the, deep, and, the, spirit, o...\n",
      "4                                              [water]\n",
      "5                                                   []\n",
      "6    [13, and, god, said, let, there, be, light, an...\n",
      "7                                                   []\n",
      "8    [14, and, god, saw, the, light, that, it, wa, ...\n",
      "9                                [from, the, darkness]\n",
      "Name: lemmatized_tokens, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\"\"\"THIS IS THE IMPROVED SCRIPT\"\"\"\n",
    "import re\n",
    "import string\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "# Load the pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "# Define a list of stop words\n",
    "stop_words = ['a', 'an', 'the', 'in', 'of', 'on']\n",
    "\n",
    "# Use a generator function to yield the data one batch at a time\n",
    "def process_text(file):\n",
    "    with open(file, encoding='utf-8-sig') as f:\n",
    "        # Use a regular expression to remove punctuation and lowercase the text\n",
    "        # Tokenize the text\n",
    "        # Use a regular expression to match numbers and special characters\n",
    "        # Use the sub() function to remove the matched characters\n",
    "        # Use a list comprehension to filter the tokens\n",
    "        # Yield the modified tokens\n",
    "        yield from (\n",
    "            tokenizer.tokenize(re.sub(f'[{string.punctuation}]', '', line.lower()))\n",
    "            for line in f\n",
    "        )\n",
    "        \n",
    "# Use the map function to apply the process_text function to each line of the file\n",
    "# Use the itertools module to flatten the list of lists\n",
    "# Create a Pandas DataFrame from the list of tokens\n",
    "df = pd.DataFrame({'tokens': chain.from_iterable(map(process_text, ['bible.txt']))})\n",
    "\n",
    "# Define a function to lemmatize the tokens\n",
    "def lemmatize_tokens(tokens):\n",
    "    \n",
    "    # initiate lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # lemmatize tokens\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    # return your lemmatized tokens\n",
    "    return lemmatized_tokens\n",
    "\n",
    "# Use the apply() method to apply the lemmatize_tokens function to the 'tokens' column\n",
    "df['lemmatized_tokens'] = df['tokens'].apply(lemmatize_tokens)\n",
    "\n",
    "# Print the first 10 rows of the 'lemmatized_tokens' column\n",
    "print(df['lemmatized_tokens'].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b25ee71c-8b9e-470a-ab2f-185cb16a3c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_book(title_path): \n",
    "    \"\"\"Read a book and return it as a string\"\"\" \n",
    "    with open(title_path,\"r\",encoding = \"utf8\") as current_file:            \n",
    "        text = current_file.read() \n",
    "        text = text.replace(\"\\n\",\"\").replace(\"\\r\",\"\") \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84a3f1ae-4b32-4920-b620-d461ce9736f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: meilisearch in /opt/conda/lib/python3.8/site-packages (0.23.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from meilisearch) (2.25.1)\n",
      "Requirement already satisfied: camel-converter[pydantic] in /opt/conda/lib/python3.8/site-packages (from meilisearch) (3.0.0)\n",
      "Requirement already satisfied: pydantic>=1.8.2 in /opt/conda/lib/python3.8/site-packages (from camel-converter[pydantic]->meilisearch) (1.10.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.8/site-packages (from pydantic>=1.8.2->camel-converter[pydantic]->meilisearch) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->meilisearch) (2021.10.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.8/site-packages (from requests->meilisearch) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->meilisearch) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->meilisearch) (1.26.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install meilisearch\n",
    "import meilisearch\n",
    "\n",
    "client = meilisearch.Client('https://ms-672dc8cf52d6-1321.sfo.meilisearch.io', 'd5282db4870cfedc6b8b4294bf1e495a08680507')\n",
    "\n",
    "# An index is where the documents are stored.\n",
    "index = client.index('bible')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b95d17c3-dfb9-4611-95b0-1bcbbd3c3900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_book(title_path): \n",
    "    \"\"\"Read a book and return it as a string\"\"\" \n",
    "    with open(title_path,\"r\",encoding = \"utf8\") as current_file:            \n",
    "        text = current_file.read() \n",
    "        text = text.replace(\"\\n\",\"\").replace(\"\\r\",\"\") \n",
    "    return text\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "# Download the file from the URL and save it to a local file\n",
    "urllib.request.urlretrieve(\"https://www.gutenberg.org/cache/epub/10/pg10.txt\", \"pg10.txt\")\n",
    "\n",
    "# Read the local file using the read_book function\n",
    "text = read_book(\"pg10.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975f7242-43ae-470c-9ae9-630070d5f179",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c25844c-f2da-41ed-9ed5-c5464a4e97ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de99d26-8a32-4e2d-b8ec-2af94d1d2f34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0665ea60-41a2-4346-923f-9f4178e83f95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "938357c4-1b78-4fd0-b00e-c35e58248a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "client = meilisearch.Client('https://ms-672dc8cf52d6-1321.sfo.meilisearch.io', 'd5282db4870cfedc6b8b4294bf1e495a08680507')\n",
    "\n",
    "# Set the URL of the book\n",
    "import requests\n",
    "\n",
    "# Send a GET request to the Gutenberg project URL\n",
    "response = requests.get('https://www.gutenberg.org/cache/epub/10/pg10.txt')\n",
    "\n",
    "# Extract the text of the book from the response\n",
    "text = response.text\n",
    "print(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d571dc46-69e4-4d37-9d2d-49e07bbf9aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Extract the text of the book from the response\n",
    "url = 'http://www.gutenberg.org/files/1342/1342-0.txt'\n",
    "data = {'text': text}\n",
    "response = requests.post(url, data=data, headers=headers, timeout=10, auth=('username', 'password'))\n",
    "\n",
    "# Set the headers for the request\n",
    "headers = {'Content-Type': 'text/plain'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aed9a482-7f75-4a09-815b-e441deecfa93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gutenberg\n",
      "eBook\n",
      "of\n",
      "The\n",
      "King\n",
      "James\n",
      "Bible\n",
      "This\n",
      "eBook\n",
      "is\n",
      "for\n",
      "the\n",
      "use\n",
      "of\n",
      "anyone\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Retrieve the text of the Bible from Project Gutenberg\n",
    "r = requests.get(r'http://www.gutenberg.org/files/10/10-0.txt')\n",
    "bible = r.text\n",
    "\n",
    "# Remove unwanted new line and tab characters from the text\n",
    "for char in [\"\\n\", \"\\r\", \"\\t\"]:\n",
    "    bible = bible.replace(char, \" \")\n",
    "\n",
    "# Subset the text to remove the Project Gutenberg introduction and footnotes\n",
    "bible = bible[14:]\n",
    "\n",
    "# Split the text into a list of words\n",
    "words = bible.split()\n",
    "\n",
    "# Print the first 15 words\n",
    "for i in range(15):\n",
    "    print(words[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e932c82-427f-459d-9ab6-5ac5d2b7956d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
