{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arcesoftware/demo/blob/main/BertTokenizer.Polars.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yi3Bs32cg0Vj",
        "outputId": "8ce1caef-b25d-4a0a-a295-cb35a0e79270"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.8/dist-packages (0.15.8)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from polars) (4.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install polars\n",
        "import torch\n",
        "import polars as pd\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import matplotlib.pyplot as plt\n",
        "import logging\n",
        "import timeit\n",
        "import string\n",
        "import re\n",
        "import itertools\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "# Use a generator function to yield the data one batch at a time\n",
        "def process_text(file):\n",
        "    with open(file, encoding='utf-8-sig') as f:\n",
        "        # Process each line of the file\n",
        "        for line in f:\n",
        "            # Use a regular expression to remove punctuation and lowercase the text\n",
        "            line = re.sub(f'[{string.punctuation}]', '', line)\n",
        "            line = line.lower()\n",
        "            # Tokenize the text\n",
        "            tokens = tokenizer.tokenize(line)\n",
        "            # Use a regular expression to match numbers and special characters\n",
        "            pattern = r'[\\d@!#]+'\n",
        "            # Use the sub() function to remove the matched characters\n",
        "            tokens = [re.sub(pattern, '', token) for token in tokens]\n",
        "            # Yield the modified tokens\n",
        "            yield tokens\n",
        "\n",
        "# Use the map function to apply the process_text function to each line of the file\n",
        "text_tokens = map(process_text, ['bible.txt'])\n",
        "\n",
        "# Use the itertools module to flatten the list of lists\n",
        "text_tokens = list(itertools.chain.from_iterable(text_tokens))\n",
        "\n",
        "# Create a Polars DataFrame from the list of tokens\n",
        "df = pd.DataFrame({'tokens': text_tokens})\n",
        "\n",
        "# Use the head() method to print the first 10 rows of the DataFrame\n",
        "print(df.head(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "budzt34MhGSt",
        "outputId": "fdee1492-7ffa-4704-fd6a-7477e02d89e9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (10, 1)\n",
            "┌─────────────────────────────┐\n",
            "│ tokens                      │\n",
            "│ ---                         │\n",
            "│ list[str]                   │\n",
            "╞═════════════════════════════╡\n",
            "│ [\"\", \"in\", ... \"earth\"]     │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
            "│ []                          │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
            "│ [\"\", \"and\", ... \"upon\"]     │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
            "│ [\"the\", \"face\", ... \"the\"]  │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
            "│ ...                         │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
            "│ [\"\", \"and\", ... \"light\"]    │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
            "│ []                          │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
            "│ [\"\", \"and\", ... \"light\"]    │\n",
            "├╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌╌┤\n",
            "│ [\"from\", \"the\", \"darkness\"] │\n",
            "└─────────────────────────────┘\n"
          ]
        }
      ]
    }
  ]
}